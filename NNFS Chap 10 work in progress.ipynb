{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electoral-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, acc:0.360, loss:1.099\n",
      "epoch:100, acc:0.400, loss:1.087\n",
      "epoch:200, acc:0.417, loss:1.077\n",
      "epoch:300, acc:0.413, loss:1.076\n",
      "epoch:400, acc:0.400, loss:1.074\n",
      "epoch:500, acc:0.400, loss:1.071\n",
      "epoch:600, acc:0.417, loss:1.067\n",
      "epoch:700, acc:0.437, loss:1.062\n",
      "epoch:800, acc:0.423, loss:1.055\n",
      "epoch:900, acc:0.387, loss:1.064\n",
      "epoch:1000, acc:0.400, loss:1.063\n",
      "epoch:1100, acc:0.443, loss:1.063\n",
      "epoch:1200, acc:0.403, loss:1.061\n",
      "epoch:1300, acc:0.390, loss:1.053\n",
      "epoch:1400, acc:0.447, loss:1.060\n",
      "epoch:1500, acc:0.420, loss:1.043\n",
      "epoch:1600, acc:0.430, loss:1.032\n",
      "epoch:1700, acc:0.403, loss:1.048\n",
      "epoch:1800, acc:0.450, loss:1.041\n",
      "epoch:1900, acc:0.427, loss:1.025\n",
      "epoch:2000, acc:0.417, loss:1.038\n",
      "epoch:2100, acc:0.463, loss:1.021\n",
      "epoch:2200, acc:0.490, loss:1.007\n",
      "epoch:2300, acc:0.450, loss:1.002\n",
      "epoch:2400, acc:0.480, loss:0.994\n",
      "epoch:2500, acc:0.467, loss:0.994\n",
      "epoch:2600, acc:0.497, loss:0.975\n",
      "epoch:2700, acc:0.443, loss:1.002\n",
      "epoch:2800, acc:0.523, loss:0.965\n",
      "epoch:2900, acc:0.533, loss:0.987\n",
      "epoch:3000, acc:0.510, loss:0.990\n",
      "epoch:3100, acc:0.497, loss:0.975\n",
      "epoch:3200, acc:0.480, loss:0.974\n",
      "epoch:3300, acc:0.470, loss:0.985\n",
      "epoch:3400, acc:0.517, loss:0.993\n",
      "epoch:3500, acc:0.540, loss:0.971\n",
      "epoch:3600, acc:0.540, loss:0.996\n",
      "epoch:3700, acc:0.483, loss:0.969\n",
      "epoch:3800, acc:0.487, loss:0.968\n",
      "epoch:3900, acc:0.523, loss:0.976\n",
      "epoch:4000, acc:0.540, loss:0.991\n",
      "epoch:4100, acc:0.553, loss:0.966\n",
      "epoch:4200, acc:0.547, loss:1.010\n",
      "epoch:4300, acc:0.493, loss:0.969\n",
      "epoch:4400, acc:0.490, loss:0.967\n",
      "epoch:4500, acc:0.513, loss:0.984\n",
      "epoch:4600, acc:0.523, loss:0.978\n",
      "epoch:4700, acc:0.570, loss:0.972\n",
      "epoch:4800, acc:0.533, loss:0.969\n",
      "epoch:4900, acc:0.503, loss:0.971\n",
      "epoch:5000, acc:0.523, loss:0.969\n",
      "epoch:5100, acc:0.543, loss:0.983\n",
      "epoch:5200, acc:0.563, loss:0.962\n",
      "epoch:5300, acc:0.570, loss:0.984\n",
      "epoch:5400, acc:0.503, loss:0.971\n",
      "epoch:5500, acc:0.520, loss:0.973\n",
      "epoch:5600, acc:0.543, loss:0.981\n",
      "epoch:5700, acc:0.560, loss:0.962\n",
      "epoch:5800, acc:0.577, loss:0.972\n",
      "epoch:5900, acc:0.530, loss:0.960\n",
      "epoch:6000, acc:0.517, loss:0.953\n",
      "epoch:6100, acc:0.543, loss:0.975\n",
      "epoch:6200, acc:0.597, loss:0.945\n",
      "epoch:6300, acc:0.543, loss:0.939\n",
      "epoch:6400, acc:0.557, loss:0.969\n",
      "epoch:6500, acc:0.597, loss:0.981\n",
      "epoch:6600, acc:0.537, loss:0.926\n",
      "epoch:6700, acc:0.547, loss:0.947\n",
      "epoch:6800, acc:0.527, loss:0.959\n",
      "epoch:6900, acc:0.543, loss:0.921\n",
      "epoch:7000, acc:0.640, loss:0.903\n",
      "epoch:7100, acc:0.557, loss:0.897\n",
      "epoch:7200, acc:0.603, loss:0.875\n",
      "epoch:7300, acc:0.603, loss:0.885\n",
      "epoch:7400, acc:0.563, loss:0.888\n",
      "epoch:7500, acc:0.620, loss:0.858\n",
      "epoch:7600, acc:0.620, loss:0.865\n",
      "epoch:7700, acc:0.597, loss:0.934\n",
      "epoch:7800, acc:0.637, loss:0.844\n",
      "epoch:7900, acc:0.620, loss:0.853\n",
      "epoch:8000, acc:0.623, loss:0.868\n",
      "epoch:8100, acc:0.587, loss:0.875\n",
      "epoch:8200, acc:0.570, loss:0.879\n",
      "epoch:8300, acc:0.623, loss:0.872\n",
      "epoch:8400, acc:0.620, loss:0.849\n",
      "epoch:8500, acc:0.597, loss:0.845\n",
      "epoch:8600, acc:0.617, loss:0.877\n",
      "epoch:8700, acc:0.613, loss:0.902\n",
      "epoch:8800, acc:0.633, loss:0.865\n",
      "epoch:8900, acc:0.597, loss:0.844\n",
      "epoch:9000, acc:0.617, loss:0.863\n",
      "epoch:9100, acc:0.630, loss:0.861\n",
      "epoch:9200, acc:0.637, loss:0.867\n",
      "epoch:9300, acc:0.627, loss:0.871\n",
      "epoch:9400, acc:0.590, loss:0.873\n",
      "epoch:9500, acc:0.587, loss:0.869\n",
      "epoch:9600, acc:0.590, loss:0.872\n",
      "epoch:9700, acc:0.580, loss:0.880\n",
      "epoch:9800, acc:0.563, loss:0.882\n",
      "epoch:9900, acc:0.583, loss:0.884\n",
      "epoch:10000, acc:0.623, loss:0.883\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\t\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "                                            \n",
    "        self.output = probabilities\n",
    "        \n",
    "    # Backward pass    \n",
    "    def backward(self, dvalues):\n",
    "    \n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    # learning rate of 1. is defalut for this optimizer\n",
    "    def __init__(self,learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "  \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases+= -self.learning_rate * layer.dbiases\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "    \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self,y_pred,y_true):\n",
    "    \n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1)\n",
    "        \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "    \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true=np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    " \n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self,inputs,y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output=self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self,dvalues,y_true):\n",
    "       \n",
    "        # Number of samples\n",
    "        samples=len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs=self.dinputs/samples\n",
    "\n",
    "    \n",
    "X, y=spiral_data(samples=100,classes=3)\n",
    "\n",
    "dense1=Layer_Dense(2,64)\n",
    "activation1=Activation_ReLU()\n",
    "dense2=Layer_Dense(64,3)\n",
    "loss_activation=Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer=Optimizer_SGD()\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss=loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions=np.argmax(loss_activation.output,axis=1)\n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch:{epoch}, '+\n",
    "              f'acc:{accuracy:.3f}, '+\n",
    "              f'loss:{loss:.3f}')\n",
    "              \n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-organ",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
